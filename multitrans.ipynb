{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXBRsqeR29QB",
    "outputId": "5a67f309-a318-409d-f73b-a867f7ea2781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import transformers\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "#from tqdm import trange\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import os\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import nlpaug\n",
    "from tcn import TemporalConvNet\n",
    "\n",
    "#DECLARE USER HERE\n",
    "#Hardeep: set to 0\n",
    "#Edmund: set to 1\n",
    "set_user = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOovjQye08DC"
   },
   "source": [
    "**SETTING ENVIRONMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yfiGuTBm5XbR"
   },
   "outputs": [],
   "source": [
    "# PATH = None\n",
    "# set_user = 3\n",
    "# if set_user == 0:\n",
    "#   PATH = '/content/gdrive/MyDrive/182proj/'\n",
    "# elif set_user == 1:\n",
    "#   PATH = '/content/gdrive/MyDrive/nlp_proj/'\n",
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.path.abspath(PATH))\n",
    "PATH = 'C:/Users/Hardeep/Desktop/nlp_proj/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjnpIe9629QH"
   },
   "source": [
    "### Data Loading/Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNQItuv7lR0E"
   },
   "source": [
    "***ONLY RUN BELOW CELL ONCE OR ELSE DATA WILL BE CORRUPTED AND YOU WILL NEED TO RESTART THE RUNTIME***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQ7ttSOn29QF",
    "outputId": "f159c3c2-18fa-4ce7-dedc-92aa22ca14d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hardeep\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_json(PATH + 'yelp_review_training_dataset.jsonl', lines=True)\n",
    "\n",
    "delete_aug_data = pd.read_csv(PATH + 'augmentations/delete_aug.csv', header=None)\n",
    "swap_aug_data = pd.read_csv(PATH + 'augmentations/swap_aug.csv', header=None)\n",
    "typo_aug_data = pd.read_csv(PATH + 'augmentations/typo_aug.csv', header=None)\n",
    "synoynm_aug_data = pd.read_csv(PATH + 'augmentations/synonym_aug.csv', header=None)\n",
    "\n",
    "delete_aug_data.columns = delete_aug_data.iloc[0]\n",
    "delete_aug_data = delete_aug_data[1:]\n",
    "delete_aug_data['stars'] = delete_aug_data['stars'].astype('int64')\n",
    "\n",
    "swap_aug_data.columns = swap_aug_data.iloc[0]\n",
    "swap_aug_data = swap_aug_data[1:]\n",
    "swap_aug_data['stars'] = swap_aug_data['stars'].astype('int64')\n",
    "\n",
    "typo_aug_data.columns = typo_aug_data.iloc[0]\n",
    "typo_aug_data = typo_aug_data[1:]\n",
    "typo_aug_data['stars'] = typo_aug_data['stars'].astype('int64')\n",
    "\n",
    "synoynm_aug_data.columns = synoynm_aug_data.iloc[0]\n",
    "synoynm_aug_data = synoynm_aug_data[1:]\n",
    "synoynm_aug_data['stars'] = synoynm_aug_data['stars'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RQzKTURx37zz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((533581, 2), (533581, 2), (533581, 2), (533581, 2), (533581, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_aug_data.shape, swap_aug_data.shape, typo_aug_data.shape, synoynm_aug_data.shape, data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl2BBNeq1Dho"
   },
   "source": [
    "TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "epr1y0M229QG"
   },
   "outputs": [],
   "source": [
    "train_test_size = 0.1\n",
    "val_test_size = 0.4\n",
    "random_state = 123\n",
    "\n",
    "test_size = 1000\n",
    "\n",
    "X = data['text'][:test_size]\n",
    "y = data['stars'][:test_size]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=train_test_size, random_state=random_state)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=val_test_size, random_state=random_state)\n",
    "\n",
    "delete_X = delete_aug_data['text'][:test_size]\n",
    "delete_y = delete_aug_data['stars'][:test_size]\n",
    "delete_X_train, delete_X_test, delete_y_train, delete_y_test = train_test_split(delete_X, delete_y, test_size=train_test_size, random_state=random_state)\n",
    "delete_X_val, delete_X_test, delete_y_val, delete_y_test = train_test_split(delete_X_test, delete_y_test, test_size=val_test_size, random_state=random_state)\n",
    "\n",
    "swap_X = swap_aug_data['text'][:test_size]\n",
    "swap_y = swap_aug_data['stars'][:test_size]\n",
    "swap_X_train, swap_X_test, swap_y_train, swap_y_test = train_test_split(swap_X, swap_y, test_size=train_test_size, random_state=random_state)\n",
    "swap_X_val, swap_X_test, swap_y_val, swap_y_test = train_test_split(swap_X_test, swap_y_test, test_size=val_test_size, random_state=random_state)\n",
    "\n",
    "typo_X = typo_aug_data['text'][:test_size]\n",
    "typo_y = typo_aug_data['stars'][:test_size]\n",
    "typo_X_train, typo_X_test, typo_y_train, typo_y_test = train_test_split(typo_X, typo_y, test_size=train_test_size, random_state=random_state)\n",
    "typo_X_val, typo_X_test, typo_y_val, typo_y_test = train_test_split(typo_X_test, typo_y_test, test_size=val_test_size, random_state=random_state)\n",
    "\n",
    "synoynm_X = synoynm_aug_data['text'][:test_size]\n",
    "synoynm_y = synoynm_aug_data['stars'][:test_size]\n",
    "synoynm_X_train, synoynm_X_test, synoynm_y_train, synoynm_y_test = train_test_split(synoynm_X, synoynm_y, test_size=train_test_size, random_state=random_state)\n",
    "synoynm_X_val, synoynm_X_test, synoynm_y_val, synoynm_y_test = train_test_split(synoynm_X_test, synoynm_y_test, test_size=val_test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XFrRb7kA29QG"
   },
   "outputs": [],
   "source": [
    "# Returns a tuple of 3 things:\n",
    "# 0: dictionary of input_ids, token_type_ids, attention_masks\n",
    "# 1: Review itself\n",
    "# 2: Rating\n",
    "class ProcessData(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, labels):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        review = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        return self.tokenizer.encode_plus(review, max_length=self.max_len, padding='max_length', return_attention_mask=True, return_tensors='pt', truncation=True), review, torch.tensor([label-1]).to(torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        review_length = len(self.data)\n",
    "        return review_length\n",
    "\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2Igm2K3Z29QG"
   },
   "outputs": [],
   "source": [
    "# TODO: DONT HARDCODE 150\n",
    "max_length = 150\n",
    "tokenized_training_data = ProcessData(X_train.to_numpy().tolist(), tokenizer, max_length, y_train.to_numpy())\n",
    "tokenized_validation_data = ProcessData(X_val.to_numpy().tolist(), tokenizer, max_length, y_val.to_numpy())\n",
    "tokenized_test_data = ProcessData(X_test.to_numpy().tolist(), tokenizer, max_length, y_test.to_numpy())\n",
    "\n",
    "tokenized_training_data_typo = ProcessData(typo_X_train.to_numpy().tolist(), tokenizer, max_length, y_train.to_numpy())\n",
    "tokenized_validation_data_typo = ProcessData(typo_X_val.to_numpy().tolist(), tokenizer, max_length, y_val.to_numpy())\n",
    "tokenized_test_data_typo = ProcessData(typo_X_test.to_numpy().tolist(), tokenizer, max_length, y_test.to_numpy())\n",
    "\n",
    "tokenized_training_data_swap = ProcessData(swap_X_train.to_numpy().tolist(), tokenizer, max_length, y_train.to_numpy())\n",
    "tokenized_validation_data_swap = ProcessData(swap_X_val.to_numpy().tolist(), tokenizer, max_length, y_val.to_numpy())\n",
    "tokenized_test_data_swap = ProcessData(swap_X_test.to_numpy().tolist(), tokenizer, max_length, y_test.to_numpy())\n",
    "\n",
    "tokenized_training_data_delete = ProcessData(delete_X_train.to_numpy().tolist(), tokenizer, max_length, y_train.to_numpy())\n",
    "tokenized_validation_data_delete = ProcessData(delete_X_val.to_numpy().tolist(), tokenizer, max_length, y_val.to_numpy())\n",
    "tokenized_test_data_delete = ProcessData(delete_X_test.to_numpy().tolist(), tokenizer, max_length, y_test.to_numpy())\n",
    "\n",
    "tokenized_training_data_synoynm = ProcessData(synoynm_X_train.to_numpy().tolist(), tokenizer, max_length, y_train.to_numpy())\n",
    "tokenized_validation_data_synoynm = ProcessData(synoynm_X_val.to_numpy().tolist(), tokenizer, max_length, y_val.to_numpy())\n",
    "tokenized_test_data_synoynm = ProcessData(synoynm_X_test.to_numpy().tolist(), tokenizer, max_length, y_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EKm8wum-AuvX"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "params = {'batch_size': batch_size,\n",
    "          'num_workers': 0}\n",
    "#Create dataloader\n",
    "loader_tokenized_training_data = torch.utils.data.DataLoader(tokenized_training_data, **params)\n",
    "loader_tokenized_training_data_typo = torch.utils.data.DataLoader(tokenized_training_data_typo, **params)\n",
    "loader_tokenized_training_data_swap = torch.utils.data.DataLoader(tokenized_training_data_swap, **params)\n",
    "loader_tokenized_training_data_delete = torch.utils.data.DataLoader(tokenized_training_data_delete, **params)\n",
    "loader_tokenized_training_data_synoynm = torch.utils.data.DataLoader(tokenized_training_data_synoynm, **params)\n",
    "\n",
    "loaders = [loader_tokenized_training_data, loader_tokenized_training_data_typo, loader_tokenized_training_data_swap, loader_tokenized_training_data_delete, loader_tokenized_training_data_synoynm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7CXp_4DhvKll"
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO MANUALLY BATCH DATA\n",
    "\n",
    "def create_data_loader_iterators(data_loaders):\n",
    "    result = []\n",
    "    for data_loader in data_loaders:\n",
    "        result += [iter(data_loader)]\n",
    "    return result\n",
    "\n",
    "def training(model, data_loaders, size, eps):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    training_acc = 0\n",
    "    \n",
    "    iters = create_data_loader_iterators(data_loaders)\n",
    "    for _ in tqdm(range(len(data_loaders[0]) - eps)):\n",
    "       \n",
    "        batch_ids_list = []\n",
    "        batch_masks_list = []\n",
    "        labels = None\n",
    "        for i in iters:\n",
    "            data = next(i)\n",
    "            batch_ids = data[0]['input_ids']\n",
    "            batch_ids = batch_ids.flatten().reshape((batch_ids.shape[0], batch_ids.shape[2]))\n",
    "            batch_ids_list.append(batch_ids.to(device))\n",
    "            batch_masks = data[0]['attention_mask']\n",
    "            batch_masks = batch_masks.flatten().reshape((batch_masks.shape[0], batch_masks.shape[2]))\n",
    "            batch_masks_list.append(batch_masks.to(device))\n",
    "            labels = data[2]\n",
    "        labels = labels.to(device)\n",
    "        output = model(batch_ids_list, batch_masks_list)\n",
    "        prediction = torch.max(output, 1)[1]\n",
    "        training_loss = criterion(output, torch.flatten(labels))\n",
    "        training_acc += torch.sum(prediction == torch.flatten(labels))\n",
    "        losses.append(training_loss.item())\n",
    "        training_loss.backward()\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        # torch.cuda.empty_cache()\n",
    "    return training_acc / size, np.mean(losses)\n",
    "\n",
    "def evaluate(model, data_loader, size):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    validation_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for vdata in tqdm(data_loader):\n",
    "            vbatch_ids = vdata[0]['input_ids']\n",
    "            vbatch_ids = vbatch_ids.flatten().reshape((vbatch_ids.shape[0], vbatch_ids.shape[2]))\n",
    "            vbatch_masks = vdata[0]['attention_mask']\n",
    "            vbatch_masks = vbatch_masks.flatten().reshape((vbatch_masks.shape[0], vbatch_masks.shape[2]))\n",
    "            vdata[2] = vdata[2].to(device)\n",
    "           \n",
    "\n",
    "            voutput = model(vbatch_ids.to(device), vbatch_masks.to(device))\n",
    "            vprediction = torch.max(voutput, 1)[1]\n",
    "            \n",
    "            vloss = criterion(voutput, torch.flatten(vdata[2]))\n",
    "            validation_acc += torch.sum(vprediction == torch.flatten(vdata[2]))\n",
    "            losses.append(vloss.item())\n",
    "    return validation_acc / size, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcqFI98h6j2i"
   },
   "source": [
    "DECLARE BATCHES IN TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RVxYedFjB-C_"
   },
   "outputs": [],
   "source": [
    "class TransTCN(nn.Module):\n",
    "    '''\n",
    "    classes: number of ratings (1-5)\n",
    "    num_augmentations: number of data augmentations\n",
    "    input_size: should be the same as num_augmentations.\n",
    "    num_channels: list of number of in/out channels for each layer. The first layer takes in_channel input_size\n",
    "                  For ex) [4, 4, 4, 4, 1] = 3 layers: input_size, 4 | 4, 4 | 4, 1 |\n",
    "    kernel_size: size of the filter in TCN\n",
    "    dropout: dropout probability for TCN\n",
    "    hidden_state: default 768, was size of the output of pretrained bert\n",
    "    is_single_bert: True or False value. If True, uses 1 bert model for all \n",
    "                    num_augmentations augementations. Else, uses num_augmentations bert \n",
    "                    models for num_augmentations augementations.\n",
    "    '''\n",
    "    def __init__(self, classes, num_augmentations, input_size, num_channels, kernel_size=2, dropout=0.3, hidden_state=768, is_single_bert=False):\n",
    "        super(TransTCN, self).__init__()\n",
    "        self.num_augmentations = num_augmentations\n",
    "        self.is_single_bert = is_single_bert\n",
    "        self.berts = []\n",
    "        if self.is_single_bert:\n",
    "            self.berts += [transformers.BertModel.from_pretrained('bert-base-uncased').to(device)]\n",
    "        else:\n",
    "            for i in range(num_augmentations):\n",
    "                self.berts.append(transformers.BertModel.from_pretrained('bert-base-uncased').to(device))\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
    "        self.finalLinear = nn.Linear(hidden_state, classes)\n",
    "\n",
    "    #Return shape (batch_size, num_channels[0], bert_output_dimension)\n",
    "    def concatBerts(self, bert_outputs):\n",
    "        concatBert = bert_outputs[0][1].unsqueeze(dim=1)\n",
    "        for i in range(1, len(bert_outputs)):\n",
    "            concatBert = torch.cat((concatBert, bert_outputs[i][1].unsqueeze(dim=1)), dim=1)\n",
    "        return concatBert\n",
    "    #(bert -> tcn) * n -> bert -> linear -> softmax\n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        bert_outputs = []\n",
    "        for i in range(self.num_augmentations):\n",
    "            if self.is_single_bert:\n",
    "                bert_outputs.append(self.berts[0](input_ids[i], attention_masks[i]))\n",
    "            else:\n",
    "                bert_outputs.append(self.berts[i](input_ids[i], attention_masks[i]))\n",
    "        output = self.concatBerts(bert_outputs)\n",
    "        output = self.tcn(output).squeeze(dim=1)\n",
    "        output = self.finalLinear(output)\n",
    "        return output\n",
    "# [4, 4, 4, 4, 4, 4, 1]\n",
    "# in_channels, outchannels\n",
    "# input_size, 4\n",
    "# 4, 4\n",
    "# 4, 4\n",
    "# 4, 1 (batch_size, 1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vmPXoo_xBmTJ"
   },
   "outputs": [],
   "source": [
    "num_tcn_layers = 3\n",
    "model = TransTCN(classes=5, num_augmentations=5, input_size=5,num_channels=[5,5] + [1], kernel_size = 2, is_single_bert=True)\n",
    "model = model.to(device)\n",
    "from torch.optim import Adam\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, correct_bias=False)\n",
    "num_epochs = 5\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,num_training_steps=len(loader_tokenized_training_data) * num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_BexD_T6CJZ-"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c204096e004c158d88834d5c6bc1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "==================================\n",
      "creatings loaders\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4304376e13b4d8f8c813a6e60d30d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=56.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 8.00 GiB total capacity; 5.17 GiB already allocated; 9.00 MiB free; 6.23 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3d8edab0a871>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# If the batch is not batch_size length, the training loop wont run that batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtraining_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;31m#validation_accuracy, validation_loss = evaluate(model, loader_tokenized_validation_data, len(X_val))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-0b466e20ab26>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(model, data_loaders, size, eps)\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_masks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mtraining_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-a2773e943726>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_masks)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_augmentations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_single_bert\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[0mbert_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mberts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mbert_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mberts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m         )\n\u001b[1;32m--> 971\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    972\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    566\u001b[0m                 )\n\u001b[0;32m    567\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    569\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    454\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    457\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     ):\n\u001b[1;32m--> 387\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    388\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"relative_key\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"relative_key_query\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 8.00 GiB total capacity; 5.17 GiB already allocated; 9.00 MiB free; 6.23 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "highest_acc = 0\n",
    "for epoch in trange(num_epochs):\n",
    "    print('Epoch: ' , str(epoch))\n",
    "    print('==================================')\n",
    "    # If the batch is not batch_size length, the training loop wont run that batch\n",
    "    eps = len(X_train) % batch_size != 0\n",
    "    training_accuracy, training_loss = training(model,loaders, len(X_train), eps)\n",
    "    #validation_accuracy, validation_loss = evaluate(model, loader_tokenized_validation_data, len(X_val))\n",
    "    \n",
    "    print('Training accuracy: ', training_accuracy )\n",
    "    print('Training loss: ', training_loss)\n",
    "    #print('Validation accuracy: ', validation_accuracy)\n",
    "    #print('Validation loss: ', validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtkxOBY_EOGZ"
   },
   "outputs": [],
   "source": [
    "print(len(np.where(y_train == 5)[0])/ len(y_train)) "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "multitrans.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
