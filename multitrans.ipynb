{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "multitrans.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXBRsqeR29QB",
        "outputId": "5a67f309-a318-409d-f73b-a867f7ea2781"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install nlpaug\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/gdrive')\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import string\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "import transformers\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "#from tqdm import trange\n",
        "from tqdm.notebook import tqdm, trange\n",
        "import os\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import nlpaug\n",
        "\n",
        "\n",
        "#DECLARE USER HERE\n",
        "#Hardeep: set to 0\n",
        "#Edmund: set to 1\n",
        "set_user = 0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: nlpaug in /usr/local/lib/python3.7/dist-packages (1.1.3)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOovjQye08DC"
      },
      "source": [
        "**SETTING ENVIRONMENT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfiGuTBm5XbR"
      },
      "source": [
        "PATH = None\n",
        "if set_user == 0:\n",
        "  PATH = '/content/gdrive/MyDrive/182proj/'\n",
        "elif set_user == 1:\n",
        "  PATH = '/content/gdrive/MyDrive/nlp_proj/'\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(PATH))\n",
        "from tcn import TemporalConvNet"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjnpIe9629QH"
      },
      "source": [
        "### Data Loading/Augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNQItuv7lR0E"
      },
      "source": [
        "***ONLY RUN BELOW CELL ONCE OR ELSE DATA WILL BE CORRUPTED AND YOU WILL NEED TO RESTART THE RUNTIME***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ7ttSOn29QF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f159c3c2-18fa-4ce7-dedc-92aa22ca14d3"
      },
      "source": [
        "data = pd.read_json(PATH + 'yelp_review_training_dataset.jsonl', lines=True)\n",
        "\n",
        "delete_aug_data = pd.read_csv(PATH + 'augmentations/delete_aug.csv', header=None)\n",
        "swap_aug_data = pd.read_csv(PATH + 'augmentations/swap_aug.csv', header=None)\n",
        "typo_aug_data = pd.read_csv(PATH + 'augmentations/typo_aug.csv', header=None)\n",
        "synoynm_aug_data = pd.read_csv(PATH + 'augmentations/synonym_aug.csv', header=None)\n",
        "\n",
        "delete_aug_data.columns = delete_aug_data.iloc[0]\n",
        "delete_aug_data = delete_aug_data[1:]\n",
        "delete_aug_data['stars'] = delete_aug_data['stars'].astype('int64')\n",
        "\n",
        "swap_aug_data.columns = swap_aug_data.iloc[0]\n",
        "swap_aug_data = swap_aug_data[1:]\n",
        "swap_aug_data['stars'] = swap_aug_data['stars'].astype('int64')\n",
        "\n",
        "typo_aug_data.columns = typo_aug_data.iloc[0]\n",
        "typo_aug_data = typo_aug_data[1:]\n",
        "typo_aug_data['stars'] = typo_aug_data['stars'].astype('int64')\n",
        "\n",
        "synoynm_aug_data.columns = synoynm_aug_data.iloc[0]\n",
        "synoynm_aug_data = synoynm_aug_data[1:]\n",
        "synoynm_aug_data['stars'] = synoynm_aug_data['stars'].astype('int64')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQzKTURx37zz"
      },
      "source": [
        "delete_aug_data.shape, swap_aug_data.shape, typo_aug_data.shape, synoynm_aug_data.shape, data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl2BBNeq1Dho"
      },
      "source": [
        "TRAIN TEST SPLIT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epr1y0M229QG"
      },
      "source": [
        "train_test_size = 0.1\n",
        "val_test_size = 0.4\n",
        "random_state = 123\n",
        "\n",
        "test_size = 1000\n",
        "\n",
        "X = data['text'][:test_size]\n",
        "y = data['stars'][:test_size]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=train_test_size, random_state=random_state)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=val_test_size, random_state=random_state)\n",
        "\n",
        "delete_X = delete_aug_data['text'][:test_size]\n",
        "delete_y = delete_aug_data['stars'][:test_size]\n",
        "delete_X_train, delete_X_test, delete_y_train, delete_y_test = train_test_split(delete_X, delete_y, test_size=train_test_size, random_state=random_state)\n",
        "delete_X_val, delete_X_test, delete_y_val, delete_y_test = train_test_split(delete_X_test, delete_y_test, test_size=val_test_size, random_state=random_state)\n",
        "\n",
        "swap_X = swap_aug_data['text'][:test_size]\n",
        "swap_y = swap_aug_data['stars'][:test_size]\n",
        "swap_X_train, swap_X_test, swap_y_train, swap_y_test = train_test_split(swap_X, swap_y, test_size=train_test_size, random_state=random_state)\n",
        "swap_X_val, swap_X_test, swap_y_val, swap_y_test = train_test_split(swap_X_test, swap_y_test, test_size=val_test_size, random_state=random_state)\n",
        "\n",
        "typo_X = typo_aug_data['text'][:test_size]\n",
        "typo_y = typo_aug_data['stars'][:test_size]\n",
        "typo_X_train, typo_X_test, typo_y_train, typo_y_test = train_test_split(typo_X, typo_y, test_size=train_test_size, random_state=random_state)\n",
        "typo_X_val, typo_X_test, typo_y_val, typo_y_test = train_test_split(typo_X_test, typo_y_test, test_size=val_test_size, random_state=random_state)\n",
        "\n",
        "synoynm_X = synoynm_aug_data['text'][:test_size]\n",
        "synoynm_y = synoynm_aug_data['stars'][:test_size]\n",
        "synoynm_X_train, synoynm_X_test, synoynm_y_train, synoynm_y_test = train_test_split(synoynm_X, synoynm_y, test_size=train_test_size, random_state=random_state)\n",
        "synoynm_X_val, synoynm_X_test, synoynm_y_val, synoynm_y_test = train_test_split(synoynm_X_test, synoynm_y_test, test_size=val_test_size, random_state=random_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFrRb7kA29QG"
      },
      "source": [
        "# Returns a tuple of 3 things:\n",
        "# 0: dictionary of input_ids, token_type_ids, attention_masks\n",
        "# 1: Review itself\n",
        "# 2: Rating\n",
        "class ProcessData(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len, labels):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.labels = labels\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        review = self.data[index]\n",
        "        label = self.labels[index]\n",
        "        return self.tokenizer.encode_plus(review, max_length=self.max_len, padding='max_length', return_attention_mask=True, return_tensors='pt', truncation=True), review, torch.tensor([label-1]).to(torch.long)\n",
        "    \n",
        "    def __len__(self):\n",
        "        review_length = len(self.data)\n",
        "        return review_length\n",
        "\n",
        "\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Igm2K3Z29QG"
      },
      "source": [
        "# TODO: DONT HARDCODE 150\n",
        "max_length = 185\n",
        "tokenized_training_data = ProcessData(X_train.to_numpy().tolist(), tokenizer, max_length, y_train.to_numpy())\n",
        "tokenized_validation_data = ProcessData(X_val.to_numpy().tolist(), tokenizer, max_length, y_val.to_numpy())\n",
        "tokenized_test_data = ProcessData(X_test.to_numpy().tolist(), tokenizer, max_length, y_test.to_numpy())\n",
        "\n",
        "tokenized_training_data_typo = ProcessData(typo_X_train.to_numpy().tolist(), tokenizer, max_length, y_train.to_numpy())\n",
        "tokenized_validation_data_typo = ProcessData(typo_X_val.to_numpy().tolist(), tokenizer, max_length, y_val.to_numpy())\n",
        "tokenized_test_data_typo = ProcessData(typo_X_test.to_numpy().tolist(), tokenizer, max_length, y_test.to_numpy())\n",
        "\n",
        "tokenized_training_data_swap = ProcessData(swap_X_train.to_numpy().tolist(), tokenizer, max_length, y_train.to_numpy())\n",
        "tokenized_validation_data_swap = ProcessData(swap_X_val.to_numpy().tolist(), tokenizer, max_length, y_val.to_numpy())\n",
        "tokenized_test_data_swap = ProcessData(swap_X_test.to_numpy().tolist(), tokenizer, max_length, y_test.to_numpy())\n",
        "\n",
        "tokenized_training_data_delete = ProcessData(delete_X_train.to_numpy().tolist(), tokenizer, max_length, y_train.to_numpy())\n",
        "tokenized_validation_data_delete = ProcessData(delete_X_val.to_numpy().tolist(), tokenizer, max_length, y_val.to_numpy())\n",
        "tokenized_test_data_delete = ProcessData(delete_X_test.to_numpy().tolist(), tokenizer, max_length, y_test.to_numpy())\n",
        "\n",
        "tokenized_training_data_synoynm = ProcessData(synoynm_X_train.to_numpy().tolist(), tokenizer, max_length, y_train.to_numpy())\n",
        "tokenized_validation_data_synoynm = ProcessData(synoynm_X_val.to_numpy().tolist(), tokenizer, max_length, y_val.to_numpy())\n",
        "tokenized_test_data_synoynm = ProcessData(synoynm_X_test.to_numpy().tolist(), tokenizer, max_length, y_test.to_numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKm8wum-AuvX"
      },
      "source": [
        "batch_size = 16\n",
        "params = {'batch_size': batch_size,\n",
        "          'num_workers': 2}\n",
        "#Create dataloader\n",
        "loader_tokenized_training_data = torch.utils.data.DataLoader(tokenized_training_data, **params)\n",
        "loader_tokenized_training_data_typo = torch.utils.data.DataLoader(tokenized_training_data_typo, **params)\n",
        "loader_tokenized_training_data_swap = torch.utils.data.DataLoader(tokenized_training_data_swap, **params)\n",
        "loader_tokenized_training_data_delete = torch.utils.data.DataLoader(tokenized_training_data_delete, **params)\n",
        "loader_tokenized_training_data_synoynm = torch.utils.data.DataLoader(tokenized_training_data_synoynm, **params)\n",
        "\n",
        "loaders = [loader_tokenized_training_data, loader_tokenized_training_data_typo, loader_tokenized_training_data_swap, loader_tokenized_training_data_delete, loader_tokenized_training_data_synoynm]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CXp_4DhvKll"
      },
      "source": [
        "\n",
        "# TODO MANUALLY BATCH DATA\n",
        "\n",
        "def create_data_loader_iterators(data_loaders):\n",
        "  result = []\n",
        "  for data_loader in data_loaders:\n",
        "    result += [iter(data_loader)]\n",
        "  return result\n",
        "\n",
        "def training(model, data_loaders, size, eps):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    training_acc = 0\n",
        "    iters = create_data_loader_iterators(data_loaders)\n",
        "    for _ in tqdm(range(len(data_loaders[0]) - eps)):\n",
        "\n",
        "      batch_ids_list = []\n",
        "      batch_masks_list = []\n",
        "      labels = None\n",
        "      stopped = False\n",
        "      for i in iters:\n",
        "\n",
        "        data = next(i)\n",
        "\n",
        "        batch_ids = data[0]['input_ids']\n",
        "        batch_ids = batch_ids.flatten().reshape((batch_ids.shape[0], batch_ids.shape[2]))\n",
        "        batch_ids_list.append(batch_ids.to(device))\n",
        "\n",
        "        batch_masks = data[0]['attention_mask']\n",
        "        batch_masks = batch_masks.flatten().reshape((batch_masks.shape[0], batch_masks.shape[2]))\n",
        "        batch_masks_list.append(batch_masks.to(device))\n",
        "        \n",
        "        labels = data[2]\n",
        "\n",
        "      labels = labels.to(device)\n",
        "      print(labels)\n",
        "      \n",
        "      output = model(batch_ids_list, batch_masks_list)\n",
        "      #print(output)\n",
        "  \n",
        "      prediction = torch.max(output, 1)[1]\n",
        "      print(prediction)\n",
        "      training_loss = criterion(output, torch.flatten(labels))\n",
        "      training_acc += torch.sum(prediction == torch.flatten(labels))\n",
        "\n",
        "      losses.append(training_loss.item())\n",
        "      training_loss.backward()\n",
        "      #nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "      \n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "      # torch.cuda.empty_cache()\n",
        "      \n",
        "    return training_acc / size, np.mean(losses)\n",
        "\n",
        "def evaluate(model, data_loader, size):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    validation_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for vdata in tqdm(data_loader):\n",
        "            vbatch_ids = vdata[0]['input_ids']\n",
        "            vbatch_ids = vbatch_ids.flatten().reshape((vbatch_ids.shape[0], vbatch_ids.shape[2]))\n",
        "            vbatch_masks = vdata[0]['attention_mask']\n",
        "            vbatch_masks = vbatch_masks.flatten().reshape((vbatch_masks.shape[0], vbatch_masks.shape[2]))\n",
        "            vdata[2] = vdata[2].to(device)\n",
        "           \n",
        "\n",
        "            voutput = model(vbatch_ids.to(device), vbatch_masks.to(device))\n",
        "            vprediction = torch.max(voutput, 1)[1]\n",
        "            \n",
        "            vloss = criterion(voutput, torch.flatten(vdata[2]))\n",
        "            validation_acc += torch.sum(vprediction == torch.flatten(vdata[2]))\n",
        "            losses.append(vloss.item())\n",
        "    return validation_acc / size, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcqFI98h6j2i"
      },
      "source": [
        "DECLARE BATCHES IN TRAINING LOOP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVxYedFjB-C_"
      },
      "source": [
        "class TransTCN(nn.Module):\n",
        "    '''\n",
        "    classes: number of ratings (1-5)\n",
        "    num_augmentations: number of data augmentations\n",
        "    input_size: should be the same as num_augmentations.\n",
        "    num_channels: list of number of in/out channels for each layer. The first layer takes in_channel input_size\n",
        "                  For ex) [4, 4, 4, 4, 1] = 3 layers: input_size, 4 | 4, 4 | 4, 1 |\n",
        "    kernel_size: size of the filter in TCN\n",
        "    dropout: dropout probability for TCN\n",
        "    hidden_state: default 768, was size of the output of pretrained bert\n",
        "    is_single_bert: True or False value. If True, uses 1 bert model for all \n",
        "                    num_augmentations augementations. Else, uses num_augmentations bert \n",
        "                    models for num_augmentations augementations.\n",
        "    '''\n",
        "    def __init__(self, classes, num_augmentations, input_size, num_channels, kernel_size=2, dropout=0.3, hidden_state=768, is_single_bert=False):\n",
        "        super(TransTCN, self).__init__()\n",
        "        self.num_augmentations = num_augmentations\n",
        "        self.is_single_bert = is_single_bert\n",
        "        self.berts = []\n",
        "        if self.is_single_bert:\n",
        "          self.berts += [transformers.BertModel.from_pretrained('bert-base-cased').to(device)]\n",
        "        else:\n",
        "          for i in range(num_augmentations):\n",
        "            self.berts.append(transformers.BertModel.from_pretrained('bert-base-cased').to(device))\n",
        "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
        "        self.finalLinear = nn.Linear(hidden_state, classes)\n",
        "\n",
        "    #Return shape (batch_size, num_channels[0], bert_output_dimension)\n",
        "    def concatBerts(self, bert_outputs):\n",
        "      concatBert = bert_outputs[0][1].unsqueeze(dim=1)\n",
        "      for i in range(1, len(bert_outputs)):\n",
        "        concatBert = torch.cat((concatBert, bert_outputs[i][1].unsqueeze(dim=1)), dim=1)\n",
        "      return concatBert\n",
        "    #(bert -> tcn) * n -> bert -> linear -> softmax\n",
        "    def forward(self, input_ids, attention_masks):\n",
        "        bert_outputs = []\n",
        "        for i in range(self.num_augmentations):\n",
        "          if self.is_single_bert:\n",
        "            bert_outputs.append(self.berts[0](input_ids[i], attention_masks[i]))\n",
        "          else:\n",
        "            bert_outputs.append(self.berts[i](input_ids[i], attention_masks[i]))\n",
        "        output = self.concatBerts(bert_outputs)\n",
        "        output = self.tcn(output).squeeze(dim=1)\n",
        "        output = self.finalLinear(output)\n",
        "        return output\n",
        "# [4, 4, 4, 4, 4, 4, 1]\n",
        "# in_channels, outchannels\n",
        "# input_size, 4\n",
        "# 4, 4\n",
        "# 4, 4\n",
        "# 4, 1 (batch_size, 1, 768)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmPXoo_xBmTJ"
      },
      "source": [
        "num_tcn_layers = 3\n",
        "model = TransTCN(classes=5, num_augmentations=5, input_size=5,num_channels=[5,5] + [1], kernel_size = 2, is_single_bert=True)\n",
        "model = model.to(device)\n",
        "from torch.optim import Adam\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5, correct_bias=False)\n",
        "num_epochs = 5\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,num_training_steps=len(loader_tokenized_training_data) * num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BexD_T6CJZ-"
      },
      "source": [
        "highest_acc = 0\n",
        "for epoch in trange(num_epochs):\n",
        "    print('Epoch: ' , str(epoch))\n",
        "    print('==================================')\n",
        "    # If the batch is not batch_size length, the training loop wont run that batch\n",
        "    eps = len(X_train) % batch_size != 0\n",
        "    training_accuracy, training_loss = training(model,loaders, len(X_train), eps)\n",
        "    #validation_accuracy, validation_loss = evaluate(model, loader_tokenized_validation_data, len(X_val))\n",
        "    \n",
        "    print('Training accuracy: ', training_accuracy )\n",
        "    print('Training loss: ', training_loss)\n",
        "    #print('Validation accuracy: ', validation_accuracy)\n",
        "    #print('Validation loss: ', validation_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtkxOBY_EOGZ"
      },
      "source": [
        "print(len(np.where(y_train == 5)[0])/ len(y_train)) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}