{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hardeep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import transformers\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "print(device)\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "#from tqdm import trange\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import os\n",
    "#from torch.optim import AdamW\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from model import TransTCN\n",
    "\n",
    "from torchtext.legacy.data import Field,LabelField,BucketIterator,TabularDataset \n",
    "from torchtext import vocab\n",
    "from nltk import word_tokenize \n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total bill for this horrible service? Over $8G...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I *adore* Travis at the Hard Rock's new Kelly ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Went in for a lunch. Steak sandwich was delici...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Today was my second out of three sessions I ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars\n",
       "0  Total bill for this horrible service? Over $8G...      1\n",
       "1  I *adore* Travis at the Hard Rock's new Kelly ...      5\n",
       "2  I have to say that this office really has it t...      5\n",
       "3  Went in for a lunch. Steak sandwich was delici...      5\n",
       "4  Today was my second out of three sessions I ha...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev = pd.read_json('yelp_review_training_dataset.jsonl',lines=True)\n",
    "rev = rev.drop(columns={'review_id'})\n",
    "rev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = 'C:/Users/Hardeep/Desktop/nlp_proj/'\n",
    "# X = rev['text']\n",
    "# y = rev['stars']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1, random_state=123)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.4, random_state=123)\n",
    "\n",
    "df,test_df = train_test_split(rev[:1000],test_size=0.25,random_state=123)\n",
    "# stratify tries to split in a manner that distribution of 'toxic' is same in both train and test\n",
    "\n",
    "train_df,val_df = train_test_split(df,test_size=0.20,random_state=123)\n",
    "\n",
    "train_df.reset_index(drop=True),val_df.reset_index(drop=True), test_df.reset_index(drop=True)\n",
    "train_df.to_csv(OUT_PATH+'train.csv',index=False)\n",
    "val_df.to_csv(OUT_PATH+'val.csv',index=False)\n",
    "test_df.to_csv(OUT_PATH+'test.csv',index=False)\n",
    "\n",
    "# X_train.to_csv(OUT_PATH+'training_data.csv', index=False)\n",
    "# X_val.to_csv(OUT_PATH+'validation_data.csv', index=False)\n",
    "# X_test.to_csv(OUT_PATH+'test_data.csv', index=False)\n",
    "\n",
    "# y_train.to_csv(OUT_PATH+'training_labels.csv', index=False)\n",
    "# y_val.to_csv(OUT_PATH+'validation_labels.csv', index=False)\n",
    "# y_test.to_csv(OUT_PATH+'test_labels.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = vocab.Vectors('glove.6B.300d.txt', OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done splitting\n",
      "done building vocav\n"
     ]
    }
   ],
   "source": [
    "text_field = Field(tokenize=word_tokenize)\n",
    "# tokenize text using word_tokenize and convert to numerical form using default parameters\n",
    "\n",
    "label_field = LabelField(dtype=torch.float) \n",
    "# useful for label string to LabelEncoding. Not useful here but doesn't hurt either\n",
    "\n",
    "fields = [('text',text_field),('stars',label_field)] \n",
    "# (column name,field object to use on that column) pair for the dictonary\n",
    "\n",
    "\n",
    "train, val, test = TabularDataset.splits(path=OUT_PATH, train='train.csv',validation='val.csv',test='test.csv', \n",
    "                                         format='csv',skip_header=True,fields=fields)\n",
    "print('done splitting')\n",
    "\n",
    "\n",
    "text_field.build_vocab(train,max_size=100000,vectors=glove,unk_init=torch.Tensor.zero_) \n",
    "\n",
    "# unk_init = torch.tensor.normal_ set the initial vectors of vocab as the glove vectors and  \n",
    "# initialize unknown words as normal distribution instead of zeros\n",
    "print('done building vocav')\n",
    "\n",
    "label_field.build_vocab(train) \n",
    "\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits((train,val,test), batch_sizes=(32,128,128),\n",
    "                                              sort_key=lambda x: len(x.comment_text),\n",
    "                                              sort_within_batch=False,\n",
    "                                              device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_neuron = len(text_field.vocab)\n",
    "embedding_dim = 10000\n",
    "rnn_kwargs = {'num_layers':4,'bidirectional':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNetwork(torch.nn.Module):\n",
    "    '''\n",
    "    Deep RNN Network which can have either one both of stacked and bi-directional properties\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,in_neuron,embedding_dim=100,hidden_size=256,out_neuron=1,m_type='rnn',drop=0.33,**kwargs):\n",
    "        '''\n",
    "        Constructor of the class which will instantiate the layers while initialisation.\n",
    "        \n",
    "        args:\n",
    "            in_neuron: input dimensions of the first layer {int}\n",
    "            embedding_dim: number of latent features you want to calculate from the input data {int} default=100\n",
    "            hidden_size: neurons you want to have in your hidden RNN layer {int} default=256\n",
    "            out_neuron: number of outputs you want to have at the end.{int} default=1\n",
    "            model: whether to use 'rnn','lstm' or 'gru' {string} \n",
    "            drop: proportion of values to dropout from the previous values randomly {float 0-1} default=0.53\n",
    "            **kwargs: any valid torch.nn.RNN, torch.nn.LSTM or torch.nn.GRU args with either 'bidirectional'=True \n",
    "                      or 'num_layers'>1\n",
    "        out: \n",
    "            return a tensor of shape {batch,out_neuron} as output \n",
    "        '''\n",
    "        super(DeepNetwork,self).__init__()\n",
    "        \n",
    "        self.m_type = m_type\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(in_neuron,embedding_dim)\n",
    "        \n",
    "        if self.m_type == 'lstm':\n",
    "            self.lstm = torch.nn.LSTM(embedding_dim,hidden_size,**kwargs)\n",
    "        elif self.m_type == 'gru':\n",
    "            self.gru = torch.nn.GRU(embedding_dim,hidden_size,**kwargs)\n",
    "        else:\n",
    "            self.rnn = torch.nn.RNN(embedding_dim,hidden_size,**kwargs) \n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(drop) \n",
    "        \n",
    "        self.dense = torch.nn.Linear(hidden_size*2,5)\n",
    "        # Last output Linear Layer will have the two Hidden States from both the directions to have the result\n",
    "        \n",
    "    \n",
    "    def forward(self,t):\n",
    "        '''\n",
    "        Activate the forward propagation\n",
    "        args:\n",
    "            t: tensors in the form of a batch {torch.tensor}\n",
    "        '''\n",
    "        t = self.dropout(self.embedding(t)) # get embeddings and dropout\n",
    "    \n",
    "        if self.m_type == 'lstm':\n",
    "            out, (hidden,_) = self.lstm(t)\n",
    "        elif self.m_type == 'gru':\n",
    "            out, hidden = self.gru(t)\n",
    "        else:\n",
    "            out, hidden = self.rnn(t)\n",
    "        # shape of rnn = (seq_len, batch, num_directions * hidden_size)\n",
    "        \n",
    "        # Concatenate the last and second last hidden. One is from backward and one is from forward\n",
    "        t = self.dropout(torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1))\n",
    "       \n",
    "        return self.dense(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = DeepNetwork(in_neuron,m_type='lstm',**rnn_kwargs)\n",
    "network.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = AdamW(network.parameters(), lr=5e-3, correct_bias=True)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,num_training_steps=len(train_iter) * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network,train_iter,optimizer,loss_fn,epoch_num):\n",
    "    '''\n",
    "    train the network using given parameters\n",
    "    args:\n",
    "        network: any Neural Network object \n",
    "        train_batch: iterator of training data\n",
    "        optimizer: optimizer for gradients calculation and updation\n",
    "        loss_fn: appropriate loss function\n",
    "        epoch_num = Epoch number so that it can show which epoch number in tqdm Bar\n",
    "    out:\n",
    "        a tuple of (average_loss,average_accuracy) of floating values for a single epoch\n",
    "    '''\n",
    "    training_acc = 0 \n",
    "    losses = []\n",
    "    network.train() # set the model in training mode as it requires gradients calculation and updtion\n",
    "    # turn off while testing using  model.eval() and torch.no_grad() block\n",
    "    \n",
    "    for batch in tqdm(train_iter,f\"Epoch: {epoch_num}\"): \n",
    "        # data will be shown to model in batches per epoch to calculate gradients per batch\n",
    "        \n",
    "        output = network(batch.text.to(device))\n",
    "        #print(output)\n",
    "        prediction = torch.max(output, 1)[1]\n",
    "        #import pdb; pdb.set_trace()\n",
    "        #print(prediction)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        training_loss = criterion(output, batch.stars.long())\n",
    "        if torch.sum(prediction == 0) == len(prediction):\n",
    "            print('all zeroes')\n",
    "        \n",
    "        #print(training_loss)\n",
    "        training_acc += torch.sum(prediction == batch.stars)\n",
    "   \n",
    "        losses.append(training_loss.item())\n",
    "        training_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(network.parameters(), max_norm=1)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    return training_acc / 1000, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9b92036ccf40caa31deb4d3de374d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "==================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5430fe136d847b19206cfe54d37ebd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch: 1'), FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "\n",
      "tensor(0.2030, device='cuda:0')\n",
      "1.5368494234587018\n",
      "==================================\n",
      "Epoch:  1\n",
      "==================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3865a56347414886a4a304cc5d9f8150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch: 2'), FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "\n",
      "tensor(0.2530, device='cuda:0')\n",
      "1.4470274260169582\n",
      "==================================\n",
      "Epoch:  2\n",
      "==================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bae0fcacdb4136a4b58f37c5581afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch: 3'), FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "all zeroes\n",
      "\n",
      "tensor(0.2580, device='cuda:0')\n",
      "1.381242545027482\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in trange(3):\n",
    "    print('Epoch: ' , str(epoch))\n",
    "    print('==================================')\n",
    "    training_accuracy, training_loss = train_network(network, train_iter,optimizer, criterion, epoch+1)\n",
    "    print(training_accuracy)\n",
    "    print(training_loss)\n",
    "    print('==================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
